# -*- coding: utf-8 -*-
"""Unemployment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zrB8N4a8qye6l3GgGAvBCE0NsoxUdwEh
"""

#!/usr/bin/env python3
"""
Unemployment in India - full analysis script
Saves cleaned data to ./outputs/cleaned_unemployment.csv
Saves plots to ./outputs/
"""

import os
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import textwrap

sns.set(style="whitegrid", context="talk")
plt.rcParams["figure.dpi"] = 120

# ---------- Config ----------
LOCAL_FILENAMES = [
    "Unemployment in India.csv",
    "unemployment.csv",
    "Unemployment_in_India.csv",
    "unemployment-in-india.csv"
]
KAGGLE_DATASET = "gokulrajkmv/unemployment-in-india"  # Kaggle slug
FALLBACK_RAW = "https://raw.githubusercontent.com/amankharwal/Website-data/master/unemployment.csv"  # mirror fallback
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(exist_ok=True)

# ---------- Helpers ----------
def try_load_local():
    for fn in LOCAL_FILENAMES:
        if Path(fn).exists():
            print(f"Loading local file: {fn}")
            return pd.read_csv(fn)
    return None

def try_download_kaggle(dataset=KAGGLE_DATASET):
    """Attempt to download dataset using Kaggle API (if kaggle installed and authenticated)."""
    try:
        from kaggle.api.kaggle_api_extended import KaggleApi
    except Exception as e:
        print("Kaggle API not available (install 'kaggle' package) or import failed:", e)
        return None
    api = KaggleApi()
    try:
        api.authenticate()
    except Exception as e:
        print("Kaggle authentication failed. Place kaggle.json in ~/.kaggle and ensure permissions are 600. Error:", e)
        return None

    dest = "data_kaggle"
    Path(dest).mkdir(exist_ok=True)
    try:
        # Try downloading all files and unzip
        api.dataset_download_files(dataset, path=dest, unzip=True)
        # find first csv
        files = list(Path(dest).glob("*.csv"))
        if files:
            print("Downloaded via Kaggle, using:", files[0])
            return pd.read_csv(files[0])
    except Exception as e:
        print("Kaggle download failed:", e)
    return None

def try_fallback_raw(url=FALLBACK_RAW):
    try:
        print("Attempting fallback raw CSV:", url)
        return pd.read_csv(url)
    except Exception as e:
        print("Fallback raw download failed:", e)
    return None

def normalize_column_mapping(df):
    """
    Map dataset columns to a standard set:
    region, date, frequency, unemployment_rate, estimated_employed, labour_participation_rate, area, longitude, latitude
    This function tries to be robust to slightly different column names.
    """
    cols = list(df.columns)
    lower_map = {c: c.lower().strip() for c in cols}
    mapping = {}
    def find(col_keywords):
        for orig, low in lower_map.items():
            for kw in col_keywords:
                if kw in low:
                    return orig
        return None

    mapping_candidates = {
        "region": ["region", "state", "area"],
        "date": ["date", "month", "time"],
        "frequency": ["frequency"],
        "unemployment_rate": ["estimated unemployment rate", "unemployment rate", "estimated_unemployment_rate", "estimated_unemployment_rate (%)", "estimated_unemployment_rate(%)", "estimated_unemployment_rate_%", "estimated_unemployment_rate(%)"],
        "estimated_employed": ["estimated employed", "estimated_employed", "estimatedemployed"],
        "labour_participation_rate": ["labour participation rate", "estimated labour participation rate", "labour_participation_rate"],
        "area": ["area"],
        "longitude": ["longitude", "long"],
        "latitude": ["latitude", "lat"]
    }

    for std, kws in mapping_candidates.items():
        found = find([k.replace(" ", "_") for k in kws] + kws)
        if found:
            mapping[found] = std

    # As fallback, try to detect the unemployment rate by looking for '%' char or 'unemployment' substring
    if "unemployment_rate" not in mapping.values():
        for orig in cols:
            if "unemployment" in orig.lower() or "%" in orig:
                mapping[orig] = "unemployment_rate"
                break

    df = df.rename(columns=mapping)
    return df

def clean_numeric_column(s):
    """Remove commas, percent signs and convert to numeric (coerce errors)."""
    if s.dtype == object:
        s = s.str.replace(",", "", regex=False).str.replace("%", "", regex=False).str.strip()
    return pd.to_numeric(s, errors="coerce")

# ---------- Load data ----------
df = try_load_local()
if df is None:
    df = try_download_kaggle()
if df is None:
    df = try_fallback_raw()
if df is None:
    raise SystemExit(textwrap.dedent("""
        Could not load dataset automatically.
        Please download the CSV from Kaggle:
        https://www.kaggle.com/datasets/gokulrajkmv/unemployment-in-india
        and place it in the working directory with one of these names:
        {}
        Or set up the Kaggle API (kaggle.json in ~/.kaggle) to allow automatic download.
    """.format(", ".join(LOCAL_FILENAMES))))

print("Raw columns:", list(df.columns))
df = normalize_column_mapping(df)
print("Mapped columns:", list(df.columns))

# ---------- Basic cleaning ----------
# strip whitespace from string columns
for c in df.select_dtypes(include=["object"]).columns:
    df[c] = df[c].str.strip()

# Parse date
if "date" in df.columns:
    df["date_parsed"] = pd.to_datetime(df["date"], errors="coerce", dayfirst=False)
else:
    # If there is no date column, try to infer a column that looks like a date
    for c in df.columns:
        try:
            parsed = pd.to_datetime(df[c], errors="coerce")
            if parsed.notna().sum() > 10:
                df["date_parsed"] = parsed
                print(f"Inferred date column from {c}")
                break
        except Exception:
            pass

# numeric conversions
for col in ["unemployment_rate", "estimated_employed", "labour_participation_rate", "longitude", "latitude"]:
    if col in df.columns:
        df[col] = clean_numeric_column(df[col])

# Standard names: ensure we have region
if "region" not in df.columns:
    # try Area or State
    for c in df.columns:
        if c.lower() in ("area", "state", "region"):
            df = df.rename(columns={c: "region"})
            break

# drop duplicates
df = df.drop_duplicates().reset_index(drop=True)

# Create year, month columns
if "date_parsed" in df.columns:
    df["year"] = df["date_parsed"].dt.year
    df["month"] = df["date_parsed"].dt.month
    df["year_month"] = df["date_parsed"].dt.to_period("M").astype(str)
else:
    df["year"] = np.nan
    df["month"] = np.nan
    df["year_month"] = np.nan

# Quick summary
print("\nData sample:")
print(df.head(5))
print("\nColumn dtypes:")
print(df.dtypes)
print("\nMissing values per column:")
print(df.isnull().sum())

# ---------- Basic EDA and Visualization ----------

# 1) National (all-region) monthly mean unemployment time series
if "unemployment_rate" in df.columns and df["year_month"].notna().sum() > 0:
    national = df.groupby("year_month", as_index=False)["unemployment_rate"].mean()
    national["year_month_dt"] = pd.to_datetime(national["year_month"], format="%Y-%m")
    plt.figure(figsize=(12,5))
    sns.lineplot(data=national, x="year_month_dt", y="unemployment_rate", marker="o")
    plt.title("Average Unemployment Rate in India (monthly)")
    plt.xlabel("Date")
    plt.ylabel("Unemployment Rate (%)")
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "national_unemployment_timeseries.png")
    plt.show()

# 2) Top 10 regions (states) by mean unemployment rate
if "region" in df.columns and "unemployment_rate" in df.columns:
    region_means = df.groupby("region", as_index=False)["unemployment_rate"].mean().sort_values("unemployment_rate", ascending=False)
    top_regions = region_means.head(10)
    plt.figure(figsize=(10,6))
    sns.barplot(x="unemployment_rate", y="region", data=top_regions)
    plt.title("Top 10 Regions by Average Unemployment Rate")
    plt.xlabel("Average Unemployment Rate (%)")
    plt.ylabel("Region")
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "top10_regions_avg_unemployment.png")
    plt.show()

# 3) Trend lines for top N states
if "region" in df.columns and "unemployment_rate" in df.columns:
    top_n = 8
    top_states = region_means.head(top_n)["region"].tolist()
    df_top = df[df["region"].isin(top_states)].copy()
    # aggregate monthly mean
    trend = df_top.groupby(["year_month", "region"], as_index=False)["unemployment_rate"].mean()
    trend["year_month_dt"] = pd.to_datetime(trend["year_month"].astype(str))
    plt.figure(figsize=(12,6))
    sns.lineplot(data=trend, x="year_month_dt", y="unemployment_rate", hue="region", marker="o")
    plt.title(f"Monthly unemployment trend for top {top_n} regions")
    plt.xlabel("Date")
    plt.ylabel("Unemployment Rate (%)")
    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "top_regions_trend.png")
    plt.show()

# 4) Heatmap: regions vs months (last 24 months if available)
if "region" in df.columns and "year_month" in df.columns and "unemployment_rate" in df.columns:
    pivot = df.pivot_table(index="region", columns="year_month", values="unemployment_rate", aggfunc="mean")
    # reduce to last 24 months for readability if many
    cols_sorted = sorted([c for c in pivot.columns if isinstance(c, str)])
    if len(cols_sorted) > 24:
        cols_keep = cols_sorted[-24:]
        pivot = pivot[cols_keep]
    plt.figure(figsize=(14, max(6, 0.2*len(pivot))))
    sns.heatmap(pivot.fillna(np.nan), cmap="YlGnBu", cbar_kws={'label': 'Unemployment Rate (%)'})
    plt.title("Heatmap of Unemployment Rate (region x year_month)")
    plt.xlabel("Year-Month")
    plt.ylabel("Region")
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "heatmap_region_month.png")
    plt.show()

# 5) Scatter: Labour participation vs unemployment (if both present)
if "labour_participation_rate" in df.columns and "unemployment_rate" in df.columns:
    plt.figure(figsize=(10,6))
    sns.scatterplot(data=df, x="labour_participation_rate", y="unemployment_rate", hue="region", alpha=0.7)
    plt.title("Labour Participation Rate vs Unemployment Rate")
    plt.xlabel("Labour Participation Rate (%)")
    plt.ylabel("Unemployment Rate (%)")
    plt.legend([],[],frameon=False)  # hide big legend if many regions
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "lpr_vs_unemployment.png")
    plt.show()

# ---------- Save cleaned file ----------
cleaned_path = OUTPUT_DIR / "cleaned_unemployment.csv"
df.to_csv(cleaned_path, index=False)
print(f"\nCleaned data saved to: {cleaned_path}")

# ---------- Quick textual summaries ----------
print("\nTop 10 regions by mean unemployment rate:")
if "unemployment_rate" in df.columns:
    print(region_means.head(10).to_string(index=False))

# Example: show spike around 2020 (COVID months) if available
if "year_month" in df.columns and "unemployment_rate" in df.columns:
    covid_period = df[df["year_month"].str.startswith("2020", na=False)]
    if not covid_period.empty:
        print("\nSummary for 2020 (COVID period) - mean unemployment by region:")
        print(covid_period.groupby("region")["unemployment_rate"].mean().sort_values(ascending=False).head(10).to_string())

print("\nDone. Plots and cleaned CSV are in the 'outputs' folder.")